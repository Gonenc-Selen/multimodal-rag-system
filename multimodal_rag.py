# -*- coding: utf-8 -*-
"""Multimodal RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xXeNNuPVOZCr9-s3LD8G20hThHXLgxlC
"""

pip install PyMuPDF

pip install faiss-cpu

import os #dosya iÃ§inde gezinme
import io #pdf'ten ayrÄ±ÅŸtÄ±rÄ±lan verileri vs bellekte tutmak iÃ§in eklenir.
import base64 #binaryden stringe, stringden binary veri Ã§evirme iÅŸlerinde kullanÄ±lÄ±r.
import numpy as np #SayÄ±sal iÅŸlemler, matris ve vektÃ¶r manipÃ¼lasyonlarÄ± iÃ§in kullanÄ±lan gÃ¼Ã§lÃ¼ bilimsel hesaplama kÃ¼tÃ¼phanesidir.
from PIL import Image # GÃ¶rselleri aÃ§mak, dÃ¼zenlemek ve kaydetmek iÃ§in kullanÄ±lan Python Imaging Libraryâ€™nin (Pillow) temel sÄ±nÄ±fÄ±dÄ±r.
import fitz  # PyMUPDF #PDF dosyalarÄ±ndan metin ve gÃ¶rsel Ã§Ä±karmak iÃ§in kullanÄ±lan hafif ve hÄ±zlÄ± bir belge iÅŸleme kÃ¼tÃ¼phanesidir.
import torch #PyTorch framework'Ã¼nÃ¼n Ã§ekirdeÄŸidir; tensÃ¶r iÅŸlemleri ve derin Ã¶ÄŸrenme modelleri iÃ§in kullanÄ±lÄ±r.
import faiss #YÃ¼ksek boyutlu vektÃ¶rler arasÄ±nda hÄ±zlÄ± benzerlik aramasÄ± ve yakÄ±n komÅŸu sorgularÄ± yapmak iÃ§in kullanÄ±lÄ±r.
import gradio as gr #Makine Ã¶ÄŸrenmesi modellerine hÄ±zlÄ±ca web arayÃ¼zÃ¼ oluÅŸturmak iÃ§in kullanÄ±lan kullanÄ±cÄ± dostu bir araÃ§tÄ±r.
from typing import List,Dict, Tuple,Optional # Python'da statik tÃ¼r ipuÃ§larÄ± vererek kodun okunabilirliÄŸini ve hatasÄ±zlÄ±ÄŸÄ±nÄ± artÄ±rmak iÃ§in kullanÄ±lÄ±r.
from dataclasses import dataclass #Otomatik olarak __init__, __repr__ gibi metodlarÄ± oluÅŸturarak sade ve temiz veri sÄ±nÄ±flarÄ± tanÄ±mlamayÄ± saÄŸlar.
from sentence_transformers import SentenceTransformer #CÃ¼mleleri vektÃ¶rel temsillere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in kullanÄ±lan, BERT tabanlÄ± hazÄ±r embedding modellerini saÄŸlayan kÃ¼tÃ¼phanedir.
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,CLIPProcessor,CLIPModel, pipeline,AutoModelForSeq2SeqLM #Hugging Face Transformers kÃ¼tÃ¼phanesinden, dil ve multimodal (metin + gÃ¶rsel) modelleri kullanmak iÃ§in gerekli bileÅŸenlerdir.
import json #JSON formatÄ±ndaki verileri okumak ve yazmak iÃ§in kullanÄ±lan yerleÅŸik bir Python modÃ¼lÃ¼dÃ¼r.
import pickle #Python nesnelerini seri hale getirip diske kaydetmek veya geri yÃ¼klemek iÃ§in kullanÄ±lÄ±r.
from tqdm import tqdm #Uzun sÃ¼ren dÃ¶ngÃ¼lere ilerleme Ã§ubuÄŸu (progress bar) ekleyerek iÅŸlemin durumunu gÃ¶rselleÅŸtiren araÃ§tÄ±r.
import warnings # UyarÄ± mesajlarÄ±nÄ± yÃ¶netmek (gizlemek, Ã¶zelleÅŸtirmek veya bastÄ±rmak) iÃ§in kullanÄ±lan yerleÅŸik Python modÃ¼lÃ¼dÃ¼r.
warnings.filterwarnings("ignore")
import re
from torch.nn.functional import normalize

# Data Structures
#dataclass_transform  # noqa: F841
@dataclass
class Document:
    """
    PDF iÃ§inden Ã§Ä±kan gÃ¶rselleri temsil eder.
    Attributes:
        text (str): Metin parÃ§asÄ±.
        page_num (int): Metnin bulunduÄŸu sayfa numarasÄ±.
        doc_name (str): PDF dosya adÄ±.
        chunk_id (int): ParÃ§a sÄ±ralama kimliÄŸi.
    """
    text: str
    page_num: int
    doc_name: str
    chunk_id: int

@dataclass
class ImageData:
    """
    PDF iÃ§inden Ã§Ä±kan gÃ¶rselleri temsil eder.
    Attributes:
        image (Image.Image): PIL Image objesi.
        page_num (int): GÃ¶rselin bulunduÄŸu sayfa.
        doc_name (str): PDF dosya adÄ±.
        image_id (int): GÃ¶rsel sÄ±ralama kimliÄŸi.
        caption (str): GÃ¶rsel aÃ§Ä±klamasÄ±.
    """

    image: Image.Image
    page_num: int
    doc_name: str
    image_id: int
    caption: str = ""

class MultimodalRAG:
    def __init__(self, use_cuda=True):
        # Text iÅŸlemenin yapÄ±lacaÄŸÄ± model indiriliyor. CÃ¼mleler vektÃ¶re Ã§evrilecek.
        self.text_model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")
        # GÃ¶rsel iÅŸleme iÃ§in kullanÄ±lacak model indiriliyor. GÃ¶rselleri vektÃ¶re Ã§evirecek.
        # Bu kÄ±sÄ±m Ã¶nceden eÄŸitilmiÅŸ modeli indiriyor. Gelen parÃ§alarÄ± vektÃ¶re Ã§eviriyor.

        #GÃ¶rsel ve vektÃ¶r iÃ§in verilen vektÃ¶rleri ortak bir uzayda karÅŸÄ±laÅŸtÄ±rabilecek vektÃ¶rler Ã¼retiyor.
        self.CLIPModel = CLIPModel.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")
        #Bu kÄ±sÄ±mda gÃ¶rselleri modele sokmak iÃ§in uygun formata getiriyor.
        self.CLIPProcessor = CLIPProcessor.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")

        # 6) Cihaz ayarlar
        self.device ="cuda" if use_cuda and torch.cuda.is_available() else "cpu"
        self.text_model.to(self.device).eval()
        self.CLIPModel.to(self.device).eval()

        self.generator = pipeline ("text2text-generation", model="google/flan-t5-large", device=0 if self.device=="cuda" else -1, max_length=512, clean_up_tokenization_spaces=True)

        # 3) FAISS metin indeksi: embedding boyutuna gÃ¶re L2 benzerliÄŸi/ En benzer vektÃ¶rÃ¼ bulur.
        self.text_index = faiss.IndexFlatL2(self.text_model.get_sentence_embedding_dimension())

        # 4) FAISS gÃ¶rsel indeksi: CLIP projeksiyon dimension
        dim_text = self.text_model.get_sentence_embedding_dimension()
        dim_img  = self.CLIPModel.config.projection_dim
        self.text_index  = faiss.IndexFlatL2(dim_text)   # L2
        self.image_index = faiss.IndexFlatIP(dim_img)    # IP (cosine iÃ§in)

        # 5) Veri deposu: Ã§Ä±karÄ±lan metin ve gÃ¶rseller listeleri
        self.documents: List[Document] = []
        self.images: List[ImageData] = []



    def clean_text(self,text: str) -> str:
        # URL'leri kaldÄ±r
        text = re.sub(r"http\S+", "", text)

        # HTML yorumlarÄ±nÄ± kaldÄ±r <!-- ... -->
        text = re.sub(r"<!--.*?-->", "", text)

        # "image", "--", "===", "___" gibi yapÄ±larÄ± kaldÄ±r
        text = re.sub(r"\bimage\b", "", text, flags=re.IGNORECASE)
        text = re.sub(r"(--+|==+|__+)", " ", text)

        # "2024 UYARI ... ." gibi yapÄ±larÄ± sadeleÅŸtir
        text = re.sub(r"\d{4}\s*UYARI.*?\.", ".", text)

        # TÃ¼rkÃ§e karakterleri koruyarak Ã¶zel karakterleri kaldÄ±r
        text = re.sub(r"[^A-Za-zÃ‡Ã§ÄÄŸÄ°Ä±Ã–Ã¶ÅÅŸÃœÃ¼0-9.,;:%&()\-\"'?!\s]", " ", text)

        # Fazla boÅŸluklarÄ± tek boÅŸluÄŸa indir
        text = re.sub(r"\s+", " ", text).strip()

        return text


    def extract_text_from_pdf(self, file_path: str, chunk_size: int = 500) -> List[Document]:
        docs: List[Document] = []
        name = os.path.basename(file_path)
        pdf = fitz.open(file_path)
        for i in range(pdf.page_count):

            raw = pdf[i].get_text()
            text= self.clean_text(raw)
            words = text.split()
            for j in range(0, len(words), chunk_size):
                chunk = " ".join(words[j:j+chunk_size])
                if chunk:
                    docs.append(Document(text=chunk, page_num=i+1, doc_name=name, chunk_id=len(docs)))
        pdf.close()
        return docs

    def extract_images_from_pdf(self, file_path: str) -> List[ImageData]:
        """
        PDF'ten gÃ¶rseller Ã§Ä±karÄ±r.
        Args:
            file_path: PDF dosya yolu.
        Returns:
            GÃ¶rsel bilgileri iÃ§eren ImageData listesi.
        """
        #Bu, elde edilecek tÃ¼m resimlerin saklanacaÄŸÄ± boÅŸ liste.Her eleman bir ImageData nesnesi olacak.
        imgs: List[ImageData] = []

        #Dosya yolundan sadece dosya adÄ±nÄ± alÄ±r.
        name = os.path.basename(file_path)

        #SayfalarÄ±n iÃ§eriÄŸine eriÅŸilebilir.
        pdf = fitz.open(file_path)

        #Sayfa sayfa gezer
        for i in range(pdf.page_count):
           #Sayfa iÃ§erisindeki tÃ¼m resimleri alÄ±r.
            for img in pdf.get_page_images(i, full=True):
                #xref PDF iÃ§indeki gÃ¶rsele verilen referans numarasÄ±dÄ±r.
                #Bu referasnla gÃ¶rselin piksel verisini eriÅŸilir.
                xref = img[0]
                # PDF iÃ§indeki bu xref gÃ¶rseli piksel formatÄ±nda (Pixmap) yÃ¼kler.
                pix = fitz.Pixmap(pdf, xref)

                #Resmin 4. kanalÄ± var mÄ± diye kontrol ediyor? Apha seffaflÄ±k kanalÄ±ymÄ±ÅŸ. Kanal sayÄ±sÄ± 4'Ã¼n altÄ±ndaysa iÅŸlemeye baÅŸlÄ±yor.
                if pix.n - pix.alpha < 4:
                    #Resimler farklÄ± formatlarda olabileceÄŸi iÃ§in png formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor byte tipinde png formatÄ±nda dosyalar oluÅŸuyor.
                    img_bytes = pix.tobytes("png")
                    #Bu gÃ¶rseller sonra Python Imaging Library objesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor.
                    pil = Image.open(io.BytesIO(img_bytes))
                    # ArdÄ±ndan ImageData iÃ§erisine ekranda gÃ¶stermek istediÄŸimiz bilgiler ekleniyor.
                    imgs.append(ImageData(image=pil, page_num=i+1, doc_name=name, image_id=len(imgs), caption=f"Image from {name}, page {i+1}"))
                #HafÄ±zayÄ± boÅŸaltmak iÃ§in sonra None deÄŸeri atanÄ±yor.
                pix = None
         #PDF dosyasÄ± kapatÄ±lÄ±yor.
        pdf.close()
        return imgs

    #VektÃ¶r oluÅŸturmak iÃ§in kullanÄ±lan fonksiyondur. OluÅŸturulan List Document burada fonksiyon iÃ§erisinde girer ve
    def embed_texts(self, docs: List[Document]) -> np.ndarray:

       """
        Document listesindeki metinleri embed vektÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
        Returns:
            2D numpy array (float32) of embeddings.
        """
       texts = [d.text for d in docs]    # Sadece metin parÃ§alarÄ±nÄ± al
       return self.text_model.encode(texts, show_progress_bar=False).astype("float32") #encode et ve vektÃ¶rleÅŸtir.


    #VektÃ¶r oluÅŸturmak iÃ§in kullanÄ±lan fonksiyondur. OluÅŸturulan List Document burada fonksiyon iÃ§erisinde girer ve
    def embed_images(self, imgs: List[ImageData]) -> np.ndarray:

       """
         GÃ¶rselleri CLIP ile embed eder
        Returns:
            2D numpy array (float32) of image embeddings.
        """

       if not imgs:
          return np.zeros(
            (0, self.CLIPModel.config.projection_dim),dtype="float32")

       # 1) GÃ¶rselleri listele
       pil_images = [imgd.image for imgd in imgs]

       # 2) Tek seferde processor
       inputs = self.CLIPProcessor(images=pil_images, return_tensors="pt",padding=True)

       # 3) Cihaz ayarÄ±
       inputs = {k: v.to(self.device) for k, v in inputs.items()}

       # 4) Tek seferde inferans
       with torch.no_grad():
          feats = self.CLIPModel.get_image_features(**inputs)  # Tensor [N, 512]

       # 5) Normalize et (unitâ€norm)
       feats = normalize(feats, dim=-1)

       # 6) CPUâ€™ya taÅŸÄ± ve NumPyâ€™ye Ã§evir
       return feats.cpu().numpy().astype("float32")  # (N, 512)

    def process_pdfs(self, paths: List[str]):
        """
        Birden fazla PDF yolunu iÅŸleyip metin ve gÃ¶rsel indekslerini gÃ¼nceller.
        """
        #YukarÄ±da yazÄ±lan fonksiyonlarÄ± burada Ã§aÄŸÄ±rarak pdf'teki gÃ¶rsel ve metinleri iÅŸler.
        #Ã–nce pdf textlerini ve imagelarÄ± Ã§Ä±karÄ±yor. ArdÄ±ndan bunlarÄ± indeksleyip liste ve image embeding'e ekler.

        self.text_index.reset()
        self.image_index.reset()

        all_docs, all_imgs = [], []

        for p in paths:
            all_docs.extend(self.extract_text_from_pdf(p))
            all_imgs.extend(self.extract_images_from_pdf(p))
        self.documents, self.images = all_docs, all_imgs

        if self.documents:
            emb = self.embed_texts(self.documents)
            self.text_index.add(emb)

        if self.images:
            emb = self.embed_images(self.images)
            faiss.normalize_L2(emb)
            self.image_index.add(emb)
        print(f"Processed {len(all_docs)} text chunks and {len(all_imgs)} images.")


    #KullanÄ±cÄ±nÄ±n yazdÄ±ÄŸÄ± metin: query -- > SonuÃ§ olarak da en yakÄ±n 5 adet sonuÃ§ bir tuple iÃ§inde dÃ¶ndÃ¼rÃ¼lÃ¼r.
    def search_text(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """
        Soru metnini embed edip en yakÄ±n k metin parÃ§asÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
        Returns list of (Document, distance).
        """
        if not self.documents:
            return []
        #KullanÄ±cÄ±dan gelen query vektÃ¶re Ã§evrilir
        q = self.text_model.encode([query]).astype("float32")
        #Text index iÃ§erisindeki en yakÄ±n 5 tane vektÃ¶rÃ¼ getirir. i --> index numarasÄ±, d --> mesafe bilgisini dÃ¶ner.
        d, i = self.text_index.search(q, k)
        # Fonksiyon ÅŸu Ã§Ä±ktÄ±yÄ± verir: [(Document(text="...", page_num=3, ...), 0.12),Document(text="...", page_num=5, ...), 0.23),]
        return [(self.documents[idx], float(dist)) for idx, dist in zip(i[0], d[0]) if idx < len(self.documents)]

    def rerank_images (self, query: str, candidates: list[ImageData], k: int):
        """
         CLIP ile text ve image Ã¶zelliklerini Ã§Ä±karÄ±p cosine benzerlikle sÄ±rala
        """
        texts = [query] * len(candidates)
        imgs  = [c.image for c in candidates]
        # Metin ve gÃ¶rsel giriÅŸlerini ayrÄ± ayrÄ± iÅŸleyin
        text_inputs = self.CLIPProcessor(text=texts, return_tensors="pt", padding=True).to(self.device)
        image_inputs = self.CLIPProcessor(images=imgs, return_tensors="pt", padding=True).to(self.device)

        with torch.no_grad():
            t_feats = self.CLIPModel.get_text_features(**text_inputs)
            i_feats = self.CLIPModel.get_image_features(**image_inputs)
            t_feats = normalize(t_feats, dim=-1)
            i_feats = normalize(i_feats, dim=-1)
            sims = (t_feats * i_feats).sum(dim=-1).cpu().numpy()
         # En yÃ¼ksek k sim ile eÅŸleÅŸenleri sÄ±rala
        idxs = np.argsort(-sims)[:k]
        return [(candidates[i], float(sims[i])) for i in idxs]

    def search_images(self, query: str, k: int = 5) -> List[Tuple[ImageData, float]]:
        """
        1) IndexFlatIP ile cosineâ€benzerliÄŸi hÄ±zlÄ±ca bul,
        2) sonra rerank_images ile en alakalÄ± k tanesini dÃ¶ndÃ¼r.
        """
        inputs = self.CLIPProcessor(text=[query], return_tensors="pt").to(self.device)
        with torch.no_grad():
          q_feat = self.CLIPModel.get_text_features(**inputs)
          q_feat = normalize(q_feat, dim=-1).cpu().numpy().astype("float32")

        # 2) FAISS(IP) aramasÄ±
        initial_k = k * 4
        initial_k = min(initial_k, len(self.images))
        # Also add a check here to prevent FAISS search with k=0 if initial_k somehow becomes 0
        if initial_k == 0:
            return []
        dists, idxs = self.image_index.search(q_feat, initial_k)
        candidates = [self.images[i] for i in idxs[0]]

        # 3) CLIP ile rerank
        return self.rerank_images(query, candidates, k)

    def generate_response(self, query: str, texts: List[Tuple[Document, float]], imgs: List[Tuple[ImageData, float]]) -> str:
        """
        Arama sonuÃ§larÄ±nÄ± birleÅŸtirerek kullanÄ±cÄ±ya okunabilir, belirli bir ÅŸablona uygun bir yanÄ±t Ã¼retir.
        Prompt yapÄ±sÄ±, ilgili metin baÄŸlamÄ±nÄ± daha fazla vurgulayacak ÅŸekilde gÃ¼ncellendi.
        """
        # Top metin parÃ§alarÄ±nÄ± hazÄ±rla (sayÄ±yÄ± 3'te tuttuk)
        text_ctx = "\n".join(f"- (p{doc.page_num}) {doc.text}" for doc, _ in texts[:1]) # 3 text chunk

        # Top gÃ¶rsel baÅŸlÄ±klarÄ±nÄ± hazÄ±rla (sayÄ±yÄ± 2'de tuttuk)
        img_ctx = "\n".join(f"- (Image p{img.page_num}) {img.caption}" for img, _ in imgs[:1]) # 2 image caption

        # Promptâ€™u oluÅŸtur - Metin baÄŸlamÄ±nÄ± daha fazla vurgulayan ve detay isteyen Ä°ngilizce yapÄ±
        prompt = (
          f"Using the following context, explain and discuss the answer to the question: '{query}'.\n" # 'explain and discuss' ekleyerek yorumlama isteÄŸimizi belirttik
          f"Refer to the text and image information provided.\n\n" # Genel referans isteÄŸi
          f"**Context:**\n"
          f"{text_ctx}\n"
          f"{img_ctx}\n\n"
          f"**Answer:**" # YanÄ±t baÅŸlangÄ±cÄ±
        )

        # DoÄŸrudan pipeline'Ä± kullanarak metin Ã¼retin. Tokenization pipeline iÃ§inde yapÄ±lÄ±r.
        # max_new_tokens 400'de, num_beams 4'te kaldÄ±.
        output = self.generator(prompt, max_new_tokens=400, num_beams=7, no_repeat_ngram_size=2)

        # Ã‡Ä±ktÄ±yÄ± dÃ¶ndÃ¼rÃ¼yoruz. YanÄ±t artÄ±k Ä°ngilizce olacaktÄ±r.
        return output[0]["generated_text"].strip()

# Sistemi baÅŸlat
rag_system = MultimodalRAG()

path= "/content/ABSA.pdf"
rag_system.process_pdfs([path])

# Arama sonuÃ§larÄ±nÄ± alÄ±n (k deÄŸerlerini Ã¶nceki ayarlarda tutalÄ±m)
txt_res = rag_system.search_text("Laguna TedarikÃ§isinin SÃ¼lfÃ¼r OranÄ± 0.71 Olarak DeÄŸiÅŸtirilmesi durumunda ne olur?", k=3)
imgs_res = rag_system.search_images("Laguna TedarikÃ§isinin SÃ¼lfÃ¼r OranÄ± 0.71 Olarak DeÄŸiÅŸtirilmesi durumunda ne olur?", k=2)

print("Text Search Results:")
if txt_res:
    for doc, dist in txt_res:
        print(f"- Doc: {doc.doc_name}, Page: {doc.page_num}, Dist: {dist:.4f}")
        print(f"  Text: {doc.text[:200]}...") # Metnin ilk 200 karakterini yazdÄ±ralÄ±m
else:
    print("No text results found.")

print("\nImage Search Results:")
if imgs_res:
    for img, dist in imgs_res:
        print(f"- Doc: {img.doc_name}, Page: {img.page_num}, Image ID: {img.image_id}, Dist: {dist:.4f}")
        print(f"  Caption: {img.caption}")
        # Ä°steÄŸe baÄŸlÄ±: GÃ¶rseli gÃ¶stermek iÃ§in
        # display(img.image.resize((100, 100))) # KÃ¼Ã§Ã¼k boyutta gÃ¶sterelim
else:
    print("No image results found.")

# ArdÄ±ndan generate_response'u Ã§alÄ±ÅŸtÄ±rÄ±n:
# print(rag_system.generate_response("Nominal value nedir?", txt_res, imgs_res))

txt = rag_system.search_text("Explain Ordinal value?", k=1)
imgs = rag_system.search_images("Explain Ordinal value?", k=1)
print(rag_system.generate_response("Explain Ordinal value?", txt, imgs))

# Gradio fonksiyonlarÄ±
def gradio_upload_pdfs(files: List[gr.File]):
    if not files:
        return "âš ï¸ LÃ¼tfen PDF dosyasÄ± yÃ¼kleyin."
    paths = []
    for f in files:
        # Kendi yÃ¶nteminle kaydetme
        temp_path = f.name if hasattr(f, 'name') else None
        if temp_path:
            paths.append(temp_path)
    try:
        rag_system.process_pdfs(paths)
        return f"âœ… {len(paths)} PDF iÅŸlendi."
    except Exception as e:
        return f"PDF iÅŸleme hatasÄ±: {e}"


def gradio_answer_question(query: str):
    if not rag_system.documents:
        return "âš ï¸ LÃ¼tfen Ã¶nce PDF yÃ¼kleyin.", None
    if not query.strip():
        return "âš ï¸ LÃ¼tfen bir soru girin.", None
    try:
        txt_res = rag_system.search_text(query, k=5)
        img_res = rag_system.search_images(query, k=2)
        resp = rag_system.generate_response(query, txt_res, img_res)
        img_path: Optional[str] = None
        if img_res:
            img_obj = img_res[0][0]
            tmp = f"/tmp/{img_obj.doc_name}_p{img_obj.page_num}_img{img_obj.image_id}.png"
            img_obj.image.save(tmp)
            img_path = tmp
        return resp, img_path
    except Exception as e:
        return f"Cevap oluÅŸturma hatasÄ±: {e}", None

with gr.Blocks() as demo:
    gr.Markdown("## ğŸ“„ Multimodal RAG: PDF + GÃ¶rsel Soru-Cevap ArayÃ¼zÃ¼")
    with gr.Tab("1ï¸âƒ£ PDF YÃ¼kle"):
        up = gr.File(file_types=[".pdf"], file_count="multiple", label="PDF YÃ¼kle")
        out1 = gr.Markdown()
        # Otomatik tetikleme
        up.change(fn=gradio_upload_pdfs, inputs=[up], outputs=[out1])
        # Elle tetikleme alternatifi
        btn1 = gr.Button("Ä°ÅŸle")
        btn1.click(fn=gradio_upload_pdfs, inputs=[up], outputs=[out1])
    with gr.Tab("2ï¸âƒ£ Soru Sor"):
        q_in = gr.Textbox(label="Sorunuz", placeholder="Nominal Value nedir?")
        btn2 = gr.Button("Sor")
        out2 = gr.Markdown(label="YanÄ±t")
        img2 = gr.Image(label="EÅŸleÅŸen GÃ¶rsel", type="filepath")
        btn2.click(fn=gradio_answer_question, inputs=[q_in], outputs=[out2, img2])

if __name__ == "__main__":
    demo.launch()

